{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFmBfS9HMNxV"
   },
   "source": [
    "\n",
    "\n",
    "# Práctica Introductoria nº 5: \"Introducción a la librería scikit-image (skimage)\" y \"Detección de estructuras geométricas con OpenCV\".\n",
    "---\n",
    "Esta última práctica introductoria se pretende mostrar el uso de la librería \"scicit-image\" para el tratamiento de imágenes en Python. Asimismo, se prácticarán algunos aspectos relacionados con la \"detección de estructuras geométricas simples\" usando OpenCV y ya explicados en el Tema 4 de la asignatura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFmBfS9HMNxV"
   },
   "source": [
    "# scikit-image\n",
    "---\n",
    "*scikit-image* (o *skimage*) es una colección de algoritmos, organizados en paquetes, para Procesamiento de Imágenes y Visión Artificial. Se trata de una librería (o biblioteca) de código abierto para el lenguaje Python, que incluye algoritmos de segmentación, transformaciones geométricas, manipulaciones entre espacios de color, análisis de formas, filtrado de imágenes, morfología mátemática, detección de características de objetos, entre otras funcionalidades. *skimage* está diseñada para interoperar con las bibliotecas numéricas y científicas NumPy y SciPy de Python. \n",
    "\n",
    "Muchas de las funciones que aparecen en *OpenCV* también lo están en *scikit-image*. Ambas son buenas alternativas: *OpenCV* tiene un propósito más general para Visión Artificial (incluyendo funciones para Visión 3D o reconocimiento facial) y *scikit-image* está más orientada específicamente a tratamiento de imágenes digitales. *OpenCV* ofrece APIs para diferentes lenguajes de programación (p.ej., C++) mientras que *scikit-image* se desarrolló específicamente para Python. Las implementaciones de los algoritmos son diferentes, y algunos trabajos indican que el rendimiento de *OpenCV* es superior.\n",
    "\n",
    "La documentación sobre esta librería puede encontrarse en:\n",
    "https://scikit-image.org/docs/stable/api/skimage.html\n",
    "\n",
    "La primera versión de *skimage* aparece en agosto de 2009. Actualmente. la última versión estable es la 0.22.0 (octubre 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afCDNdTrOeZP"
   },
   "source": [
    "El paquete principal de *skimage* solo proporciona algunas utilidades para convertir entre tipos de datos de imagen. Para poder usar la mayoría de las funciones de procesamiento de imágenes, es necesario importar alguno(s) de los siguientes subpaquetes específicos relacionados con el problema que se quiera resolver. Entre ellos están los siguientes: \n",
    "\n",
    "*color* : conversiones entre espacios de color y funciones relacionadas con el uso del color.  \n",
    "*data* : contiene imágenes de prueba y datos de ejemplo.  \n",
    "*draw* : permite dibujar primitivas geométricas (p.ej., líneas) que trabajan con matrices NumPy.  \n",
    "*exposure* : permiten realizar ajustes sobre el brillo de la imagen (p.ej., realizar la ecualización de un histograma).  \n",
    "*feature* : permiten realizar la detección y extracción de características (p.ej., esquinas, características de análisis de texturas).  \n",
    "*filters* : realización de filtrados sobre imágenes (p.ej., detección de bordes).  \n",
    "*morphology* : para la aplicación de operaciones morfológicas a las imágenes (p.ej., erosión, apertura, ...).  \n",
    "*restoration* : para la restauración de imágenes (p.ej., deconvolución, eliminación de ruidos).   \n",
    "*draw* : para dibujar sobre las imágenes.  \n",
    "*io* : para leer, escribir y mostrar imágenes y vídeos.  \n",
    "*measure* : para calcular medidas de características de objetos en imágenes (p.ej. distancias, perímetros, ...).  \n",
    "*metrics* : métricas correspondientes a las imágenes (p.ej., distancias, similitudes,...).  \n",
    "*segmentation* : algoritmos para segmentar (o dividir) una imagen en múltiples regiones.   \n",
    "*transform* : lleva a cabo diferentes tipos de transformaciones sobre las imágenes (p.ej., transformaciones geométricas, Fourier, ...).  \n",
    "*graph* : para manejar una imagen como un grafo (p.ej., generar grafo de etiquetas de una imagen).  \n",
    "*util* : contiene utilidades diversas (p.ej., convertir los valores de los píxeles de una imagen en valores flotantes entre 0 y 1, recortar una región de una imagen, ...).  \n",
    "*viewer* : una sencilla interfaz gráfica de usuario para visualizar resultados y explorar valores de parámetros.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFmBfS9HMNxV"
   },
   "source": [
    "# Ejemplos de uso de *scikit-image*\n",
    "\n",
    "A continuación, se codifican algunos ejemplos de uso de esta librería y se proponen algunos ejercicios con los que pretendemos que el alumno aprenda a aplicarla en la resolución de problemas sobre procesamiento de imágenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Detección de elementos en imágenes\n",
    "\n",
    "Se proponen algunos ejemplos carga de imágenes en las que se detectan bordes, contornos y esquinas, finalmente se muestran los correspondientes resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1580403324213,
     "user": {
      "displayName": "Jose Miguel Buenaposada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBA1qJk-lV1jzIfvz6oKTMtjJvtZw3QlLDRMYquUA=s64",
      "userId": "11852372497014329596"
     },
     "user_tz": -60
    },
    "id": "8ylko6elYChA",
    "outputId": "419106d0-e39c-4edf-9251-f03b257fcd8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se aplica el filtrado de bordes de Sobel a una imagen ejemplo\n",
    "from skimage import data, io, filters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title ('Imagen original')\n",
    "image = data.coins()\n",
    "io.imshow(image)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title ('Imagen de bordes con Sobel')\n",
    "# Ojo: produce combinación de filtrado vertical y horizontal\n",
    "# Pueden aplicarse separadamente ambos filtrados mediante: skimage.filters.sobel_h y skimage.filters.sobel_v\n",
    "edges = filters.sobel(image)    \n",
    "io.imshow(edges, cmap='gray')\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvKyscaKT9gU"
   },
   "source": [
    "*Ejercicio:* Probar algunos de los filtrados paso alto y paso bajo explicados (o no) en clase con diferentes imágenes de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, io, filters\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.morphology import disk\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title ('Imagen original')\n",
    "image = data.page()\n",
    "io.imshow(image)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title ('Filtrado de mediana 5x5:')\n",
    "mediana = filters.median(image, disk(5))    \n",
    "io.imshow(mediana)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvKyscaKT9gU"
   },
   "source": [
    "*Ejercicio:* Aplicar un filtrado de Canny a la imagen de 'cameraman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1580403417674,
     "user": {
      "displayName": "Jose Miguel Buenaposada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBA1qJk-lV1jzIfvz6oKTMtjJvtZw3QlLDRMYquUA=s64",
      "userId": "11852372497014329596"
     },
     "user_tz": -60
    },
    "id": "kSHmxjdmYZSG",
    "outputId": "1a00ab58-54b7-44c7-c721-d78a8cf5d6b3"
   },
   "outputs": [],
   "source": [
    "# Se aplica el filtrado de contornos de Canny a una imagen ejemplo\n",
    "from skimage import data, io, feature\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title ('Imagen original')\n",
    "image = io.imread('cameraman.png')\n",
    "im_gris=rgb2gray(image)\n",
    "print(image.shape)\n",
    "io.imshow(im_gris)\n",
    "print(im_gris.shape)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title ('Imagen de contornos con Canny')\n",
    "contours = feature.canny(im_gris) # Probar: feature.canny(im_gris, sigma=3) \n",
    "io.imshow(contours)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvKyscaKT9gU"
   },
   "source": [
    "*Ejercicio:* Escribir un código que aplique filtrados morfológicos a una imagen binaria sintética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.morphology import erosion, dilation, square\n",
    "cuadrado = np.array([[0, 0, 0, 0, 0],[0, 1, 1, 1, 0],[0, 1, 1, 1, 0],[0, 1, 1, 1, 0],[0, 0, 0, 0, 0]], dtype=np.uint8)\n",
    "print('cuadrado')\n",
    "print(cuadrado)\n",
    "print()\n",
    "\n",
    "erosionada= erosion(cuadrado, square(3))\n",
    "print('erosionada')\n",
    "print(erosionada)\n",
    "print()\n",
    "\n",
    "dilatada= dilation(cuadrado, square(3))\n",
    "print('dilatada')\n",
    "print(dilatada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1580403417674,
     "user": {
      "displayName": "Jose Miguel Buenaposada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBA1qJk-lV1jzIfvz6oKTMtjJvtZw3QlLDRMYquUA=s64",
      "userId": "11852372497014329596"
     },
     "user_tz": -60
    },
    "id": "kSHmxjdmYZSG",
    "outputId": "1a00ab58-54b7-44c7-c721-d78a8cf5d6b3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Se aplica el detector de esquinas de Harris a una imagen ejemplo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title ('Imagen original')\n",
    "image = data.checkerboard()\n",
    "io.imshow(image)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title ('Imagen de esquinas de Harris')\n",
    "corners = feature.corner_peaks(feature.corner_harris(image), min_distance=5)\n",
    "plt.plot(corners[:, 1], corners[:, 0], 'r.')\n",
    "io.imshow(image)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Extraer propiedades sobre la forma\n",
    "\n",
    "Se proponen algunos ejemplos de extracción de propiedades sobre las formas contenidas en una imagen binaria sintética usando la librería *measure* y la función *regionprops*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importan librerias\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.draw import ellipse\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.transform import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crean y dibujan dos elipses sintéticas de diferentes caracteristicas en una matriz\n",
    "\n",
    "image = np.zeros((600, 600))\n",
    "rr, cc = ellipse(300, 350, 100, 220)\n",
    "image[rr, cc] = 1\n",
    "\n",
    "image = rotate(image, angle=15, order=0)\n",
    "\n",
    "rr, cc = ellipse(100, 100, 60, 50)\n",
    "image[rr, cc] = 1\n",
    "\n",
    "label_img = label(image)\n",
    "regions = regionprops(label_img)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "\n",
    "for props in regions:\n",
    "    y0, x0 = props.centroid\n",
    "    orientation = props.orientation\n",
    "    x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length\n",
    "    y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length\n",
    "    x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length\n",
    "    y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length\n",
    "\n",
    "    ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5)\n",
    "    ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5)\n",
    "    ax.plot(x0, y0, '.g', markersize=15)\n",
    "\n",
    "    minr, minc, maxr, maxc = props.bbox\n",
    "    bx = (minc, maxc, maxc, minc, minc)\n",
    "    by = (minr, minr, maxr, maxr, minr)\n",
    "    ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "\n",
    "ax.axis((0, 600, 600, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = regionprops_table(label_img, properties=('centroid',\n",
    "                                                 'orientation',\n",
    "                                                 'minor_axis_length',\n",
    "                                                 'major_axis_length'))\n",
    "pd.DataFrame(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer algunas características de forma contenidas en una imagen real convertida a binaria\n",
    "from skimage import measure, data\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a binary image\n",
    "image = data.coins()\n",
    "gris = rgb2gray(image)\n",
    "binaria = gris > 128\n",
    "plt.imshow(binaria, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellenar agujeros en la imagen binaria\n",
    "from scipy import ndimage\n",
    "\n",
    "binaria_rellena = ndimage.binary_fill_holes(binaria)\n",
    "plt.imshow(binaria_rellena, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "binaria_rellena[0:20, 0:150] = 0\n",
    "plt.imshow(binaria_rellena, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicar calcular medidas de las regiones detectadas\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage import morphology\n",
    "import pandas as pd\n",
    "\n",
    "image = binaria_rellena\n",
    "label_img = label(image)\n",
    "regions = regionprops(label_img)\n",
    "print('numero componentes conexas = ', len(regions))\n",
    "\n",
    "cleaned_image = morphology.remove_small_objects(label_img, min_size=30)\n",
    "regions = regionprops(cleaned_image)\n",
    "print('numero objetos = ', len(regions))\n",
    "\n",
    "# propiedades seleccionadas de los objetos\n",
    "props = regionprops_table(cleaned_image, properties=('centroid',\n",
    "                                                 'area',\n",
    "                                                 'perimeter',\n",
    "                                                 'eccentricity'))                       \n",
    "pd.DataFrame(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Comparativa entre imágenes\n",
    "\n",
    "Se proponen algunos ejemplos de comparativas entre pares de imágenes usando la librería *metrics*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1580403417674,
     "user": {
      "displayName": "Jose Miguel Buenaposada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBA1qJk-lV1jzIfvz6oKTMtjJvtZw3QlLDRMYquUA=s64",
      "userId": "11852372497014329596"
     },
     "user_tz": -60
    },
    "id": "kSHmxjdmYZSG",
    "outputId": "1a00ab58-54b7-44c7-c721-d78a8cf5d6b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.metrics import structural_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title ('Primera imagen')\n",
    "img1 = data.camera()\n",
    "io.imshow(img1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title ('Segunda imagen')\n",
    "img2 = data.moon()\n",
    "io.imshow(img2)\n",
    "io.show()\n",
    "\n",
    "# Compute SSIM\n",
    "ssim = structural_similarity(img1, img2, multichannel=True)\n",
    "\n",
    "print(\"Structural Similarity Index: \", ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFmBfS9HMNxV"
   },
   "source": [
    "# Detección de estructuras geométricas con OpenCV\n",
    "\n",
    "A continuación, se codifican algunos métodos de detección de estructuras geométricas, explicados en el Tema 4, usando OpenCV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Transformada de Hough\n",
    "\n",
    "La transformada de Hough es una técnica para la detección de figuras geométricas en imágenes digitales. Esta técnica, que fue patentada en 1962 por Paul Hough, es mayormente usada en el campo de Visión Artificial. Con la transformada de Hough es posible encontrar todo tipo de figuras geométicas que puedan ser expresadas matemáticamente en coordenadas paramétricas, tales como rectas, circunferencias o elipses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformada de Hough para detección de líneas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# se lee y se visualiza la imagen\n",
    "image = cv2.imread('moviles.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la imagen a niveles de gris\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(gray,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los parámetros para la aplicación de Canny (extracción de bordes previa a Hough)\n",
    "low_threshold = 130\n",
    "high_threshold = 190\n",
    "edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "plt.imshow(edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los parametros de la transformada de Hough\n",
    "rho = 1\n",
    "theta = np.pi/180\n",
    "threshold = 60\n",
    "min_line_length = 50\n",
    "max_line_gap = 5\n",
    "\n",
    "line_image = np.copy(image) # Se hace una copia de la imagen para dibujar en ella las lineas detectadas por Hough\n",
    "\n",
    "lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap) # Aplicación Hough\n",
    "\n",
    "# Se recorren las lineas detectadas y se dibujan sobre la imagen copia\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),2)\n",
    "        \n",
    "plt.imshow(line_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Transformada de Hough para detección de círculos\n",
    "\n",
    "Se ilustra cómo aplicar aplicar la transformada de Hough para detectar las circunferencias en una imagen de iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1580403417674,
     "user": {
      "displayName": "Jose Miguel Buenaposada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBA1qJk-lV1jzIfvz6oKTMtjJvtZw3QlLDRMYquUA=s64",
      "userId": "11852372497014329596"
     },
     "user_tz": -60
    },
    "id": "kSHmxjdmYZSG",
    "outputId": "1a00ab58-54b7-44c7-c721-d78a8cf5d6b3"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# se carga y visualiza una imagen de un ojo\n",
    "ojo = cv2.imread(\"eye_image2.jpg\")\n",
    "print(ojo.shape)\n",
    "plt.title('Imagen ocular (formato RGB)')\n",
    "plt.imshow(cv2.cvtColor(ojo, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ojo_gris = cv2.cvtColor(ojo, cv2.COLOR_BGR2GRAY) # se convierte a gris\n",
    "# se binariza adecuadamente la imagen para que los pasos sucesivos funcionen bien\n",
    "umbral, ojo_bin = cv2.threshold(ojo_gris, 100, 255, cv2.THRESH_BINARY)\n",
    "plt.title('Imagen ocular (binaria)')\n",
    "plt.imshow(ojo_bin,cmap=\"gray\",vmin=0,vmax=255)\n",
    "print(umbral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se buscan los contornos para, a partir de ellos, sacar la pupila\n",
    "ojo_gris_suaviz = cv2.blur(ojo_gris, (3,3))\n",
    "ojo_bordes = cv2.Canny(ojo_gris_suaviz, 180, 200)\n",
    "plt.title('contornos')\n",
    "plt.imshow(ojo_bordes,cmap=\"gray\",vmin=0,vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se aplica la transformada de Hough para círculos\n",
    "c = cv2.HoughCircles(ojo_bordes, cv2.HOUGH_GRADIENT, 1, 20, param1 = 25,\n",
    "              param2 = 20, minRadius = 1, maxRadius = 50)\n",
    "\n",
    "# se muestran sobre la imagen original los dos círculos más relevantes\n",
    "print(c)\n",
    "for l in c: \n",
    "    for circle in l:\n",
    "        center = (int(circle[0]), int(circle[1]))\n",
    "        radius = int(circle[2])\n",
    "        print(center)\n",
    "        print(radius)\n",
    "        cv2.circle(ojo, center, radius, (0, 255, 0), 2)\n",
    "\n",
    "radius2 = int (radius/3)\n",
    "cv2.circle(ojo, center, radius2, (0, 255, 0), 2)      \n",
    "\n",
    "plt.imshow(cv2.cvtColor(ojo, cv2.COLOR_BGR2RGB))\n",
    "plt.show\n",
    "\n",
    "cv2.imwrite('ojo_procesado.png', ojo)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de homografías con RANSAC y emparejamiento de Puntos de interés\n",
    "RANdom SAmple Consensus (RANSAC) es un método iterativo para calcular los parámetros de un modelo matemático de un conjunto de datos observados que contiene valores atípicos. Un ejemplo de uso de RANSAC es el emparejamiento de puntos de interés extraídos en dos imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MIN_MATCH_COUNT = 4\n",
    "img2 = cv.imread('meninas_museo2.jpeg',0)          # imagen de query\n",
    "img1 = cv.imread('meninas_plantilla.jpeg',0) # imagen de train\n",
    "img1 = cv.resize(img1, (2*img1.shape[1],2*img1.shape[0]))\n",
    "print(img1.shape)\n",
    "print(img2.shape)\n",
    "\n",
    "# Iniciar el detector y descriptor de puntos de interés ORB (Oriented FAST and Rotated BRIEF).\n",
    "# ORB es básicamente una combinación del detector FAST y el descriptor BRIEF.\n",
    "# ORB funciona igual de bien que SIFT en la detección (y mejor que SURF), siendo dos órdenes de magnitud más rápido. \n",
    "detector = cv.ORB_create()\n",
    "descriptor = cv.ORB_create()\n",
    "\n",
    "# Encontrar los puntos de interés y los descriptores\n",
    "kp1 = detector.detect(img1)\n",
    "kp1, des1 = descriptor.compute(img1,kp1)\n",
    "kp2 = detector.detect(img2) \n",
    "kp2, des2 = descriptor.compute(img2,kp2)\n",
    "\n",
    "# Matcher de puntos de interés BFMatcher con distancia L1 para descriptores binarios\n",
    "matcher = cv.BFMatcher(normType=cv.NORM_L1)\n",
    "matches = matcher.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Guardar todos los \"matches buenos\"\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, ransacReprojThreshold=10)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    h,w = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv.perspectiveTransform(pts,M)\n",
    "    img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)\n",
    "else:\n",
    "    print( \"No se han encontrado matches suficientes - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "    matchesMask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_params = dict(matchColor = (0,255,0), # dibujar los matches en color verde\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # dibujar solo los inliers (dentro de una región)\n",
    "                   flags = 2)\n",
    "img3 = cv.drawMatches(img1.copy(),kp1,img2.copy(),kp2,good,None,**draw_params)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img3, 'gray'),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrlD3W2TXehl"
   },
   "source": [
    "### Umbralización adaptativa\n",
    "\n",
    "En la \"umbralización global\" se establece un único umbral para todos los píxeles de la imagen. En la \"umbralización adaptativa\" (o variable) se calculan los valores de umbral a nivel de cada píxel en las imágenes. Dichos umbrales locales se calculan  en base a las características de la vecindad de cada píxel evaluado. Ello permitirá segmentar, de manera más precisa, imágenes que contengan fondos con distintos niveles de grises o incluso imágenes con iluminación no uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('sudoku.jpg', 0) # lectura de una imagen en niveles de gris\n",
    "\n",
    "img = cv2.medianBlur(img,5) # se atenúa el ruido mediante un filtro de mediana\n",
    "\n",
    "# Se comparan los resultados de tres umbralizados aplicados a la imagen original: uno global,  \n",
    "# otro adaptativo basado en la media de una vecindad y el último de tipo adaptativo basado en \n",
    "# una suma ponderada gaussiano de los píxeles vecinos. \n",
    "ret, img_umbrGlobal = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "img_umbrAdaptMedia = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\n",
    "\n",
    "img_umbrAdapGaussiano = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "\n",
    "# Se visualiza la imagen original y los tres umbralizados\n",
    "titles = ['Imagen Original', 'Umbralización Global (v = 127)',\n",
    "            'Umbr. Adap. (Media)', 'Umbr. Adap. (Gauss)']\n",
    "images = [img, img_umbrGlobal, img_umbrAdaptMedia, img_umbrAdapGaussiano]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: Comparar el umbralizado de Otsu con una umbralzación adaptativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero probamos el método de Otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv.imread('constitucion-de-la-pepa-cadiz-1812_d9a3556a.jpg',0)\n",
    "\n",
    "ret, bin_img = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "ret, outs_img = cv.threshold(img, 127, 255, cv.THRESH_OTSU)\n",
    "\n",
    "plt.imshow(img,'gray');\n",
    "plt.title(\"Original\");\n",
    "plt.show()\n",
    "plt.imshow(bin_img,'gray');\n",
    "plt.title(\"Gris 127\");\n",
    "plt.show()\n",
    "plt.imshow(outs_img,'gray');\n",
    "plt.title(\"Otsu\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, probamos dos métodos de umbralización adaptativa: primero usando como umbral el valor medio de la vecindad de píxeles (menos una constante) cv.ADAPTIVE_THRESH_MEAN_C y después se usa como umbral la suma ponderada según una gaussiana de la vecindad de píxeles (menos una constante C) cv.ADAPTIVE_THRESH_GAUSSIAN_C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv.imread('constitucion-de-la-pepa-cadiz-1812_d9a3556a.jpg',0)\n",
    "ret,img_thres = cv.threshold(img, 127, 255, cv.THRESH_OTSU)\n",
    "img_med = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 21, 30)\n",
    "img_gauss = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 21, 30)\n",
    "\n",
    "plt.imshow(img,'gray');\n",
    "plt.title(\"Original\");\n",
    "plt.show()\n",
    "plt.imshow(img_thres,'gray');\n",
    "plt.title(\"Outsu\");\n",
    "plt.show()\n",
    "plt.imshow(img_med,'gray');\n",
    "plt.title(\"Adaptativo 21 media\");\n",
    "plt.show()\n",
    "plt.imshow(img_gauss,'gray');\n",
    "plt.title(\"Adaptativo 21 Gausiana\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de regiones de alto contraste con MSER\n",
    "MSER (Maximally Stable Extremal Regions) es un detector de regiones estables de tamaño máximo. \n",
    "El algoritmo trabajo analizando las variaciones de intensidad en una imagen, e identifica regiones conexas con similares valores de intensidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from random import random\n",
    "from colorsys import hsv_to_rgb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('girasoles.jpeg', 0)\n",
    "output = np.zeros((img.shape[0],img.shape[1],3),dtype=np.uint8)\n",
    "\n",
    "mser = cv2.MSER_create(delta=10, max_variation=0.15, min_area=100, max_area=15000)\n",
    "polygons = mser.detectRegions(img)\n",
    "for polygon in polygons[0]:\n",
    "    colorRGB = hsv_to_rgb(random(),1,1)\n",
    "    colorRGB = tuple(int(color*255) for color in colorRGB)\n",
    "    output = cv2.fillPoly(output,[polygon],colorRGB)\n",
    "\n",
    "plt.figure(figsize=(15,15))    \n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP995NaerTif3mfP3XjF/kp",
   "collapsed_sections": [],
   "name": "urjc_va_000_intro_a_python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
